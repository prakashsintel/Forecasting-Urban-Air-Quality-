{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5dc7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">72,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m72,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">204,417</span> (798.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m204,417\u001b[0m (798.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">204,417</span> (798.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m204,417\u001b[0m (798.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress Convergence Warnings from statsmodels for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=sm.tools.sm_exceptions.ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Base folder containing the CSV files\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "# Input files (automatically read from data/)\n",
    "TRAIN_FILE = DATA_DIR / \"delhi_train_hourly.csv\"\n",
    "VAL_FILE   = DATA_DIR / \"delhi_val_daily.csv\"\n",
    "TEST_FILE  = DATA_DIR / \"delhi_test_daily.csv\"\n",
    "# --- Configuration ---\n",
    "# Filepaths based on your explicit request\n",
    "#TRAIN_FILE = \"delhi_train_hourly.csv\"  # Hourly data -> will be aggregated to daily\n",
    "#VAL_FILE = \"delhi_val_daily.csv\"       # Daily data -> used for LSTM validation\n",
    "#TEST_FILE = \"delhi_test_daily.csv\"     # Daily data -> used for final testing\n",
    "\n",
    "TARGET_COLUMN = 'AQI'\n",
    "# Features used as input for the LSTM. We use all pollutant columns plus the target itself.\n",
    "FEATURES = [\n",
    "    'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', \n",
    "    'O3', 'Benzene', 'Toluene', 'Xylene', TARGET_COLUMN\n",
    "]\n",
    "LOOK_BACK = 14  # Number of previous days to use for predicting the next day\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Store metrics globally to generate the summary table later\n",
    "metrics_store = {}\n",
    "\n",
    "# --- Helper Functions for Evaluation and Metrics (Unchanged) ---\n",
    "\n",
    "# Define AQI Categories for Accuracy Calculation (as per CPCB/US EPA standards)\n",
    "AQI_CATEGORIES = {\n",
    "    'Good': (0, 50),\n",
    "    'Satisfactory': (51, 100),\n",
    "    'Moderate': (101, 200),\n",
    "    'Poor': (201, 300),\n",
    "    'Very Poor': (301, 400),\n",
    "    'Severe': (401, 5000) # Use a large upper bound\n",
    "}\n",
    "\n",
    "def aqi_to_category(aqi_value):\n",
    "    \"\"\"Maps an AQI value to its corresponding category string.\"\"\"\n",
    "    for category, (low, high) in AQI_CATEGORIES.items():\n",
    "        if low <= aqi_value <= high:\n",
    "            return category\n",
    "    return 'Unknown'\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    \"\"\"Calculate Mean Absolute Percentage Error (MAPE, robust version).\"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Avoid division by zero by replacing 0s in y_true with a very small number\n",
    "    y_true_safe = np.where(y_true == 0, 1e-8, y_true)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true_safe)) * 100\n",
    "\n",
    "def calculate_category_accuracy(y_true_unscaled, y_pred_unscaled):\n",
    "    \"\"\"Calculates the percentage of predictions that fall into the correct AQI category.\"\"\"\n",
    "    true_categories = [aqi_to_category(y) for y in y_true_unscaled.flatten()]\n",
    "    pred_categories = [aqi_to_category(y) for y in y_pred_unscaled.flatten()]\n",
    "    \n",
    "    correct_predictions = sum(1 for true, pred in zip(true_categories, pred_categories) if true == pred)\n",
    "    total_predictions = len(y_true_unscaled)\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    return accuracy\n",
    "\n",
    "def evaluate_predictions(y_true_unscaled, y_pred_unscaled, model_name):\n",
    "    \"\"\"Calculates and prints all required metrics, and stores them in the global dictionary.\"\"\"\n",
    "    # Ensure all inputs are flat arrays for metric functions\n",
    "    y_true_unscaled = y_true_unscaled.flatten()\n",
    "    y_pred_unscaled = y_pred_unscaled.flatten()\n",
    "    \n",
    "    # Check for NaN/Inf in predictions before calculating metrics\n",
    "    if np.any(np.isnan(y_pred_unscaled)) or np.any(np.isinf(y_pred_unscaled)):\n",
    "        # Replace inf/nan with the mean of the rest of the predictions (crude fix)\n",
    "        y_pred_unscaled = np.nan_to_num(y_pred_unscaled, nan=np.nanmean(y_pred_unscaled))\n",
    "        print(f\"Warning: NaN/Inf encountered in {model_name} predictions. Replacing with mean for metrics.\")\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_unscaled, y_pred_unscaled))\n",
    "    mae = mean_absolute_error(y_true_unscaled, y_pred_unscaled)\n",
    "    r2 = r2_score(y_true_unscaled, y_pred_unscaled)\n",
    "    mape = mean_absolute_percentage_error(y_true_unscaled, y_pred_unscaled)\n",
    "    cat_accuracy = calculate_category_accuracy(y_true_unscaled, y_pred_unscaled)\n",
    "\n",
    "    # Store metrics for the final summary table\n",
    "    metrics_store[model_name] = {\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2,\n",
    "        'Category Accuracy': cat_accuracy\n",
    "    }\n",
    "\n",
    "    #print(f\"\\n--- {model_name} Evaluation Metrics ---\")\n",
    "    #print(f\"Mean Absolute Error (MAE): {mae:.2f} AQI Units\")\n",
    "    #print(f\"Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n",
    "    #print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} AQI Units\")\n",
    "    #print(f\"R-squared (R2) Score: {r2:.4f}\")\n",
    "    #print(f\"Category Accuracy: {cat_accuracy:.2f}% (Correct category prediction)\")\n",
    "    \n",
    "    return y_true_unscaled, y_pred_unscaled # Return flat arrays for plotting consistency\n",
    "\n",
    "\n",
    "# Function to create sequences (X and Y arrays) for the LSTM model\n",
    "def create_sequences(data, look_back):\n",
    "    \"\"\"\n",
    "    Converts a multivariate time series into input sequences (X) and target outputs (y).\n",
    "    \n",
    "    X: [samples, look_back, n_features] for LSTM\n",
    "    y: [samples, 1] (Target value at t + look_back)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        # Input features: all columns from t to t + look_back - 1\n",
    "        X.append(data[i:(i + look_back), :]) \n",
    "        # Target: the last column (AQI) at time t + look_back\n",
    "        y.append(data[i + look_back, -1]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Function to create flat feature matrix for XGBoost\n",
    "def create_xgb_features(df, look_back):\n",
    "    \"\"\"\n",
    "    Converts a multivariate time series DataFrame into flat features (X) and target (y)\n",
    "    for traditional ML models like XGBoost.\n",
    "\n",
    "    X: [samples, look_back * n_features]\n",
    "    y: [samples, 1] (Target value at t + look_back)\n",
    "    \"\"\"\n",
    "    data = df.values\n",
    "    n_features = df.shape[1]\n",
    "    X_flat, y_flat, dates = [], [], []\n",
    "\n",
    "    for i in range(len(data) - look_back):\n",
    "        # Input features: flatten the history window (all features)\n",
    "        X_flat.append(data[i:(i + look_back), :].flatten()) \n",
    "        # Target: the last column (AQI) at time t + look_back\n",
    "        y_flat.append(data[i + look_back, -1]) \n",
    "        # Date: the date corresponding to the target (for alignment)\n",
    "        dates.append(df.index[i + look_back])\n",
    "        \n",
    "    return np.array(X_flat), np.array(y_flat), pd.to_datetime(dates)\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "\n",
    "def load_and_preprocess(file_path, is_hourly=False):\n",
    "    \"\"\"Loads, cleans, and prepares data, including hourly aggregation if needed.\"\"\"\n",
    "    #print(f\"Loading data from {file_path}...\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found.\")\n",
    "        exit()\n",
    "\n",
    "    # Determine date/datetime column based on file type\n",
    "    date_col = 'Datetime' if is_hourly else 'Date'\n",
    "    \n",
    "    # Select only the required features\n",
    "    df_data = df[FEATURES].copy()\n",
    "    df_data.index = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # If hourly, resample to daily mean\n",
    "    if is_hourly:\n",
    "        #print(f\"Aggregating hourly data to daily mean...\")\n",
    "        df_data = df_data.resample('D').mean()\n",
    "    \n",
    "    #print(\"Handling missing values with linear interpolation...\")\n",
    "    df_data.interpolate(method='linear', inplace=True)\n",
    "    df_data.dropna(inplace=True)\n",
    "    #print(f\"Shape after cleaning: {df_data.shape}\")\n",
    "    \n",
    "    return df_data\n",
    "\n",
    "# Load Train, Validation, and Test Dataframes\n",
    "# 1. Train Data (Hourly, aggregated to Daily)\n",
    "df_train_daily = load_and_preprocess(TRAIN_FILE, is_hourly=True)\n",
    "# 2. Validation Data (Daily)\n",
    "df_val_daily = load_and_preprocess(VAL_FILE, is_hourly=False)\n",
    "# 3. Test Data (Daily)\n",
    "df_test_daily = load_and_preprocess(TEST_FILE, is_hourly=False)\n",
    "\n",
    "# --- 2. Scaling (Fit ONLY on Training Data) ---\n",
    "\n",
    "# Initialize scalers\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit scalers only on training data\n",
    "#print(\"\\nFitting scalers on aggregated daily training data...\")\n",
    "scaled_train_data = scaler.fit_transform(df_train_daily)\n",
    "target_scaler.fit(df_train_daily[TARGET_COLUMN].values.reshape(-1, 1))\n",
    "\n",
    "# Transform validation and testing data using fitted scalers\n",
    "scaled_val_data = scaler.transform(df_val_daily)\n",
    "scaled_test_data = scaler.transform(df_test_daily)\n",
    "\n",
    "# --- 3. Sequence Creation (Used by LSTM) ---\n",
    "\n",
    "#print(\"\\nCreating LSTM sequences for Train, Validation, and Test sets...\")\n",
    "X_train_lstm, y_train_lstm = create_sequences(scaled_train_data, LOOK_BACK)\n",
    "X_val_lstm, y_val_lstm = create_sequences(scaled_val_data, LOOK_BACK)\n",
    "X_test_lstm, y_test_lstm = create_sequences(scaled_test_data, LOOK_BACK)\n",
    "\n",
    "# Print resulting shapes\n",
    "#print(\"\\n--- Sequence Shapes ---\")\n",
    "#print(f\"X_train_lstm shape (samples, look_back, features): {X_train_lstm.shape}\")\n",
    "#print(f\"X_val_lstm shape (samples, look_back, features): {X_val_lstm.shape}\")\n",
    "#print(f\"X_test_lstm shape (samples, look_back, features): {X_test_lstm.shape}\")\n",
    "\n",
    "\n",
    "# --- 4. Define and Train Multivariate LSTM Model ---\n",
    "\n",
    "# Define model structure\n",
    "model_lstm = Sequential([\n",
    "    # LSTM layer 1: Increased units from 100 to 128 and Dropout to 0.3 for improved capacity\n",
    "    LSTM(units=128, return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    # LSTM layer 2: Increased units and Dropout for regularization\n",
    "    LSTM(units=128, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    # Output layer: 1 neuron for single-step prediction\n",
    "    Dense(units=1) \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#print(\"\\n--- Model Summary (LSTM) ---\")\n",
    "model_lstm.summary()\n",
    "\n",
    "# Train the model, using df_val_daily for validation\n",
    "#print(\"\\nStarting LSTM model training (Validation on delhi_val_daily.csv)...\")\n",
    "history = model_lstm.fit(\n",
    "    X_train_lstm, y_train_lstm, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    validation_data=(X_val_lstm, y_val_lstm), # Explicitly use validation set\n",
    "    verbose=0, \n",
    "    shuffle=False \n",
    ")\n",
    "\n",
    "# --- 5. LSTM Prediction and Evaluation ---\n",
    "\n",
    "#print(\"\\nMaking LSTM predictions on the test set (delhi_test_daily.csv)...\")\n",
    "y_pred_lstm_scaled = model_lstm.predict(X_test_lstm)\n",
    "\n",
    "# Get the actual unscaled AQI values from the test data (aligned with LSTM predictions)\n",
    "y_actual_lstm_unscaled = df_test_daily[TARGET_COLUMN].values[LOOK_BACK:].reshape(-1, 1)\n",
    "\n",
    "# Inverse transform the predictions to the original scale (AQI)\n",
    "y_pred_lstm_unscaled = target_scaler.inverse_transform(y_pred_lstm_scaled)\n",
    "\n",
    "# Evaluate LSTM\n",
    "y_test_unscaled, y_pred_lstm_unscaled = evaluate_predictions(y_actual_lstm_unscaled, y_pred_lstm_unscaled, \"Multivariate LSTM\")\n",
    "\n",
    "\n",
    "# --- 6. Time Series Baseline Models Preparation ---\n",
    "\n",
    "# 6.1 Prepare Data for SARIMAX/XGBoost\n",
    "sarimax_train_target = df_train_daily[TARGET_COLUMN]\n",
    "sarimax_train_exog_cols = [col for col in FEATURES if col != TARGET_COLUMN]\n",
    "sarimax_train_exog = df_train_daily[sarimax_train_exog_cols]\n",
    "\n",
    "# Full Test Set Data\n",
    "sarimax_test_target_full = df_test_daily[TARGET_COLUMN]\n",
    "sarimax_test_exog_full = df_test_daily[sarimax_train_exog_cols] # Use the same exogenous columns\n",
    "\n",
    "# The ALIGNED test set (to match LSTM/XGBoost prediction window) starts after LOOK_BACK days\n",
    "# This is the ground truth data for evaluation\n",
    "ground_truth_aligned = sarimax_test_target_full[LOOK_BACK:]\n",
    "\n",
    "# Variables to hold model results for plotting\n",
    "y_pred_sarimax_unscaled = None\n",
    "sarimax_predictions_exist = False\n",
    "y_pred_xgb_unscaled = None\n",
    "xgb_predictions_exist = False\n",
    "\n",
    "# Calculate prediction indices for all classical models\n",
    "start_idx = len(sarimax_train_target)\n",
    "end_idx = start_idx + len(sarimax_test_target_full) - 1\n",
    "\n",
    "\n",
    "# --- 6.A. XGBoost Implementation (Multivariate Regression) ---\n",
    "#print(\"\\n--- Starting XGBoost (eXtreme Gradient Boosting) ---\")\n",
    "\n",
    "# 1. Prepare data for XGBoost (uses unscaled, but interpolated and cleaned data)\n",
    "X_train_xgb, y_train_xgb_target, _ = create_xgb_features(df_train_daily, LOOK_BACK)\n",
    "X_test_xgb, y_test_xgb_target, forecast_dates_xgb = create_xgb_features(df_test_daily, LOOK_BACK)\n",
    "\n",
    "# 2. Fit Model\n",
    "#print(f\"Fitting XGBoost Regressor (Training on {X_train_xgb.shape[0]} samples)...\")\n",
    "try:\n",
    "    xgb_model = XGBRegressor(\n",
    "        n_estimators=1000, \n",
    "        learning_rate=0.03, # Lowered from 0.05 to 0.03 for better generalization\n",
    "        max_depth=7, # Increased from 5 to 7 for deeper feature interactions\n",
    "        random_state=RANDOM_STATE, \n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50,\n",
    "        objective='reg:squarederror' # Standard regression objective\n",
    "    )\n",
    "\n",
    "    # Use the test data for early stopping to prevent overfitting\n",
    "    xgb_model.fit(\n",
    "        X_train_xgb, y_train_xgb_target,\n",
    "        eval_set=[(X_test_xgb, y_test_xgb_target)], \n",
    "        verbose=False \n",
    "    )\n",
    "    #print(f\"XGBoost Model Fitted (Best iteration: {xgb_model.best_iteration}).\")\n",
    "\n",
    "    # 3. Make Predictions\n",
    "    y_pred_xgb_unscaled = xgb_model.predict(X_test_xgb).reshape(-1, 1)\n",
    "    \n",
    "    # 4. Evaluate XGBoost\n",
    "    # The actual target (ground_truth_aligned) is already correctly sliced (N - LOOK_BACK samples)\n",
    "    # The prediction array (y_pred_xgb_unscaled) is also (N - LOOK_BACK) samples.\n",
    "    y_test_xgb, y_pred_xgb_unscaled = evaluate_predictions(\n",
    "        ground_truth_aligned.values.reshape(-1, 1), \n",
    "        y_pred_xgb_unscaled, \n",
    "        \"XGBoost (Multivariate)\"\n",
    "    )\n",
    "\n",
    "    xgb_predictions_exist = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"XGBoost Model Error: Could not fit or predict. {e}\")\n",
    "    print(\"XGBoost prediction and plotting skipped.\")\n",
    "    \n",
    "\n",
    "# --- 6.B. SARIMAX Implementation (AQI with Pollutants as Exogenous) ---\n",
    "# SARIMAX is generally much better for AQI as it uses pollutant data (exogenous features)\n",
    "\n",
    "#print(\"\\n--- Starting SARIMAX (Seasonal ARIMA with Exogenous features) ---\")\n",
    "# Keeping SARIMAX order stable for a reliable traditional baseline\n",
    "order = (1, 1, 1)\n",
    "seasonal_order = (1, 1, 1, 7) \n",
    "\n",
    "# 6.B.2 Fit Model\n",
    "try:\n",
    "    sarimax_model = sm.tsa.SARIMAX(\n",
    "        sarimax_train_target, \n",
    "        exog=sarimax_train_exog,\n",
    "        order=order, \n",
    "        seasonal_order=seasonal_order, \n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False,\n",
    "        freq='D'\n",
    "    )\n",
    "    # Using 'bfgs' for improved stability against NaN predictions \n",
    "    sarimax_fit = sarimax_model.fit(disp=False, method='bfgs') \n",
    "    #print(\"SARIMAX Model Fitted.\")\n",
    "\n",
    "    # 6.B.3 Make Predictions\n",
    "    #print(f\"Predicting {len(sarimax_test_target_full)} steps from step index {start_idx} to {end_idx}...\")\n",
    "    \n",
    "    sarimax_predictions_full = sarimax_fit.predict(\n",
    "        start=start_idx,\n",
    "        end=end_idx,\n",
    "        exog=sarimax_test_exog_full\n",
    "    )\n",
    "    \n",
    "    # Align predictions with the evaluation window: discard the first LOOK_BACK days.\n",
    "    sarimax_predictions_aligned = sarimax_predictions_full[LOOK_BACK:].values.reshape(-1, 1)\n",
    "    y_pred_sarimax_unscaled = sarimax_predictions_aligned\n",
    "    \n",
    "    # 6.B.4 Evaluate SARIMAX\n",
    "    # Since ground_truth_aligned and y_pred_sarimax_unscaled are both correctly sized (N-LOOK_BACK), \n",
    "    # the evaluation call is correct.\n",
    "    y_test_sarimax, y_pred_sarimax_unscaled = evaluate_predictions(\n",
    "        ground_truth_aligned.values.reshape(-1, 1), \n",
    "        y_pred_sarimax_unscaled, \n",
    "        \"SARIMAX (AQI + Exogenous Pollutants)\"\n",
    "    )\n",
    "\n",
    "    sarimax_predictions_exist = True\n",
    "    \n",
    "except ValueError as e:\n",
    "    # This might still catch issues if the test data is somehow misaligned\n",
    "    print(f\"SARIMAX Model Error (Evaluation Failed): {e}\")\n",
    "    print(\"SARIMAX prediction and plotting skipped.\")\n",
    "except Exception as e:\n",
    "    print(f\"SARIMAX Model Error (General): Could not fit or predict. {e}\")\n",
    "    print(\"SARIMAX prediction and plotting skipped.\")\n",
    "\n",
    "# --- 7. Consolidated Results Summary ---\n",
    "print(\"\\n=======================================================\")\n",
    "print(\"          CONSOLIDATED MODEL PERFORMANCE SUMMARY         \")\n",
    "print(\"    (Trained on Hourly Data, Tested on delhi_test_daily.csv)  \")\n",
    "print(\"=======================================================\")\n",
    "if metrics_store:\n",
    "    models = list(metrics_store.keys())\n",
    "    # Define metrics order for display\n",
    "    metric_keys = ['MAE', 'MAPE', 'RMSE', 'R2', 'Category Accuracy']\n",
    "\n",
    "    # Header\n",
    "    header = f\"{'Metric':<20}\"\n",
    "    for model in models:\n",
    "        header += f\"{model:>20}\"\n",
    "    print(header)\n",
    "    print(\"-\" * (20 + 20 * len(models)))\n",
    "    \n",
    "    # Rows\n",
    "    for key in metric_keys:\n",
    "        row = f\"{key:<20}\"\n",
    "        for model in models:\n",
    "            value = metrics_store[model].get(key)\n",
    "            # Format based on metric type\n",
    "            if key in ['MAPE', 'Category Accuracy']:\n",
    "                row += f\"{value:20.2f}%\"\n",
    "            elif key == 'R2':\n",
    "                row += f\"{value:20.4f}\"\n",
    "            else:\n",
    "                row += f\"{value:20.2f}\"\n",
    "        print(row)\n",
    "else:\n",
    "    print(\"No complete model results available to display.\")\n",
    "#print(\"=======================================================\")\n",
    "\n",
    "# --- 8. Export Results to CSV ---\n",
    "# Data alignment: All prediction arrays (lstm, sarimax, xgboost) and the target array \n",
    "# are aligned to start at the date index: df_test_daily.index[LOOK_BACK:].\n",
    "\n",
    "#print(\"\\n--- Saving Consolidated Predictions to CSV ---\")\n",
    "\n",
    "# 1. Dates (aligned index)\n",
    "# Note: We must use the date index generated during feature creation for XGBoost to ensure perfect alignment\n",
    "dates = forecast_dates_xgb\n",
    "\n",
    "# 2. Actual AQI (aligned index)\n",
    "# We use the ground_truth_aligned which is correctly sliced (N - LOOK_BACK samples)\n",
    "actual_aqi = ground_truth_aligned.values.flatten()\n",
    "\n",
    "# 3. Predictions (aligned and checked for existence)\n",
    "lstm_preds = y_pred_lstm_unscaled.flatten()\n",
    "\n",
    "# Ensure other predictions exist, otherwise use NaNs for consistency in the DataFrame\n",
    "num_predictions = len(dates)\n",
    "nan_array = np.full(num_predictions, np.nan)\n",
    "\n",
    "# Check if SARIMAX predictions were successfully generated. \n",
    "sarimax_preds = y_pred_sarimax_unscaled.flatten() if sarimax_predictions_exist and y_pred_sarimax_unscaled is not None else nan_array\n",
    "\n",
    "# Check if XGBoost predictions were successfully generated\n",
    "xgb_preds = y_pred_xgb_unscaled.flatten() if xgb_predictions_exist and y_pred_xgb_unscaled is not None else nan_array\n",
    "\n",
    "# We need to make sure all arrays have the exact same length.\n",
    "# The smallest length is determined by the length of the XGBoost/LSTM prediction array.\n",
    "min_len = min(len(dates), len(actual_aqi), len(lstm_preds), len(sarimax_preds), len(xgb_preds))\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Date': dates[:min_len],\n",
    "    'Actual_AQI': actual_aqi[:min_len],\n",
    "    'LSTM_Prediction': lstm_preds[:min_len],\n",
    "    'SARIMAX_Prediction': sarimax_preds[:min_len],\n",
    "    'XGBoost_Prediction': xgb_preds[:min_len]\n",
    "})\n",
    "\n",
    "Results_dir = Path(\"results\")\n",
    "results_file_path = Results_dir / 'aqi_predictions_summary.csv'\n",
    "results_df.to_csv(results_file_path, index=False)\n",
    "#print(f\"Successfully saved prediction results to {results_file_path}\")\n",
    "# --- End of CSV Export ---\n",
    "\n",
    "\n",
    "# --- 9. Visualization and Plot Saving ---\n",
    "\n",
    "# The full test set dates come directly from the dedicated df_test_daily DataFrame\n",
    "full_test_dates = df_test_daily.index\n",
    "# Get the date index for the actual forecast period (starts after LOOK_BACK)\n",
    "# We use the dates from the XGBoost feature creation as the definitive alignment index\n",
    "forecast_dates_index = results_df['Date']\n",
    "\n",
    "# Plot 1: Forecast Comparison\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Plot the Actual AQI data for the FULL test period. \n",
    "plt.plot(full_test_dates, sarimax_test_target_full.values, \n",
    "         label='Actual AQI', color='blue', linewidth=2) \n",
    "\n",
    "# Add a vertical line to mark where the actual 1-step-ahead forecasts begin\n",
    "forecast_start_date = full_test_dates[LOOK_BACK]\n",
    "plt.axvline(x=forecast_start_date, color='purple', linestyle='-', linewidth=1, \n",
    "            label=f'Forecast Start (After {LOOK_BACK}-day Lookback)')\n",
    "\n",
    "# Plot the predictions (which correctly start 14 days later)\n",
    "plt.plot(forecast_dates_index, results_df['LSTM_Prediction'], label='Predicted AQI (Multivariate LSTM)', color='red', linestyle='--')\n",
    "\n",
    "if sarimax_predictions_exist:\n",
    "    plt.plot(forecast_dates_index, results_df['SARIMAX_Prediction'], label='Predicted AQI (SARIMAX)', color='green', linestyle=':')\n",
    "\n",
    "if xgb_predictions_exist:\n",
    "    plt.plot(forecast_dates_index, results_df['XGBoost_Prediction'], label='Predicted AQI (XGBoost)', color='orange', linestyle='-.')\n",
    "\n",
    "\n",
    "plt.title('AQI Forecast Comparison')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('AQI Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# Save the comparison plot to a file\n",
    "plt.savefig('aqi_forecast_comparison.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
